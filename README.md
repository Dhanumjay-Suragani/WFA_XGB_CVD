# ğŸ«€ WFAâ€‘XGB CVD Prediction

> **A researchâ€‘grade, endâ€‘toâ€‘end machine learning pipeline** for **Weighted Feature Augmented (WFA) XGBoost**â€“based prediction of cardiovascular disease (CVD), with advanced feature engineering, SHAP explainability, and experiment tracking.

---

## âœ¨ Why This Project?
Cardiovascular disease remains one of the leading causes of mortality worldwide. This project explores a **novel WFAâ€‘XGB modeling strategy** that enhances predictive performance by:

- Augmenting features using statistical & SHAPâ€‘driven signals
- Applying focal loss to handle class imbalance
- Integrating explainability for clinical interpretability
- Maintaining full reproducibility and experiment traceability

---

## ğŸš€ Key Features

- ğŸ” **Endâ€‘toâ€‘End ML Pipeline**  
  Data loading â†’ splitting â†’ feature engineering â†’ training â†’ evaluation

- ğŸ“Š **Baseline Models**  
  Logistic Regression, Random Forest, and tuned XGBoost baselines

- ğŸ§  **WFAâ€‘XGB Model**  
  Custom Weighted Feature Augmented XGBoost with focal loss support

- ğŸ” **Explainability (SHAP)**  
  Global & local SHAP values, feature importance, augmented weights

- ğŸ§ª **Experiment Tracking**  
  CSVâ€‘based logging of ablations, MI scores, and SHAP summaries

- ğŸŒ **Deploymentâ€‘Ready Skeleton**  
  API entrypoint, deployment utilities, inference stubs

- â™»ï¸ **Reproducibility**  
  Centralized configs, path management, seeded randomness

---

## ğŸ› ï¸ Tech Stack

- **Language**: ğŸ Python 3.10+  
- **ML & Data**:  
  `numpy`, `pandas`, `scikitâ€‘learn`, `xgboost`, `shap`, `joblib`, `matplotlib`

- **Experimentation**: ğŸ““ Jupyter Notebooks  
- **API / Deployment**:  
  `app/api.py`, `src/deployment/`

- **Tooling**:  
  `pip`, Git, GitHub

---

## ğŸ“‚ Project Structure

```text
wfa_xgb_cvd_prediction/
â”œâ”€â”€ app/                     # API entrypoint
â”‚   â””â”€â”€ api.py
â”œâ”€â”€ data/                    # Datasets
â”‚   â”œâ”€â”€ feature_metadata.json
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ heart_disease_processed.csv
â”‚       â””â”€â”€ heart_Verified.csv
â”œâ”€â”€ experiments/             # Experiment outputs
â”œâ”€â”€ models/                  # Trained model artifacts
â”œâ”€â”€ notebooks/               # Research notebooks
â”œâ”€â”€ reports/                 # Figures & tables
â”œâ”€â”€ src/                     # Core ML source code
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ explainability/
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Prerequisites
- Python 3.10+ (3.11 recommended)
- Git
- (Optional) virtualenv / conda

---

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Dhanumjay-Suragani/WFA_XGB_CVD.git
cd wfa_xgb_cvd_prediction
```

---

### 3ï¸âƒ£ Create & Activate Virtual Environment

```bash
python -m venv .venv

# Windows (PowerShell)
.venv\Scripts\Activate.ps1

# macOS / Linux
source .venv/bin/activate
```

---

### 4ï¸âƒ£ Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables

No mandatory environment variables are hardâ€‘coded.

**Recommended for production:**

```env
WFA_ENV=dev
WFA_LOG_LEVEL=INFO
WFA_DATA_ROOT=./data
```

Create a `.env` file locally (not committed).

---

## â–¶ï¸ Run the Project

### ğŸ““ Run Notebooks

```bash
pip install jupyter
jupyter notebook
```

Open notebooks in this order:

1. 01_data_overview.ipynb
2. 02_baseline_models.ipynb
3. 03_shap_analysis.ipynb
4. 04_feature_augmentation.ipynb
5. 05_wfa_xgb_training.ipynb
6. 06_evaluation.ipynb
7. 07_ablation_study.ipynb

---

### ğŸ‹ï¸ Run Training Pipeline (when implemented)

```bash
python -m src.training.train_pipeline
```

---

### ğŸŒ Run API (when implemented)

```bash
uvicorn app.api:app --reload
```

---

## ğŸ§ª Testing

```bash
pip install pytest
pytest -q
```

---

## ğŸ“¦ Deployment Notes

- Treat `models/` as generated artifacts
- Reâ€‘generate models using:

```bash
python -m src.training.train_pipeline
```

- Store production models in object storage (S3, GCS) or a model registry
- Containerize API + models
- Add centralized logging in `src/utils/logger.py`

---

## ğŸ› ï¸ Common Errors & Fixes

âŒ **ModuleNotFoundError: No module named 'xgboost'**  
â¡ï¸ `pip install -r requirements.txt`

âŒ **ImportError: cannot import name 'XGBClassifier'**  
â¡ï¸ Pin version:
```text
xgboost==1.7.6
```

âŒ **MemoryError during SHAP/XGBoost**  
â¡ï¸ Reduce `n_estimators`, subsample data, or shrink SHAP background

âŒ **Notebooks cannot find data**  
â¡ï¸ Verify `src/config/paths.py`

âŒ **Inconsistent results**  
â¡ï¸ Ensure `src/utils/seed.py` is used

---

## ğŸ¤ Contributing

```bash
git checkout -b feature/my-feature
```

- Follow PEP 8
- Add docstrings
- Add/update tests
- Commit format:  
  `feat:`, `fix:`, `docs:`, `refactor:`, `chore:`

---

## ğŸ“œ License

This project is licensed under the LICENSE file.

---

## â­ Acknowledgements

- XGBoost Team
- SHAP Contributors
- UCI Heart Disease Dataset

---

ğŸ’¡ *If you find this project useful, please consider giving it a star on GitHub!*

